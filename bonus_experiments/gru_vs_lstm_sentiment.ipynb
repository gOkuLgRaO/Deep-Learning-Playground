{
 "cells": [
  {
   "cell_type": "code",
   "id": "6a6d3d01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T05:42:09.548559Z",
     "start_time": "2025-09-01T05:42:04.742286Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from frameworks.lstm_sentiment import build_vocab, preprocess_data\n",
    "from torchtext.datasets import IMDB\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import save_plot"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "c86bb9f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T05:42:24.136420Z",
     "start_time": "2025-09-01T05:42:24.131723Z"
    }
   },
   "source": [
    "# GRU Model\n",
    "class GRUClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=128, num_classes=2):\n",
    "        super(GRUClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, h_n = self.gru(x)\n",
    "        return self.fc(h_n[-1])"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "7a85a873",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T05:42:31.216134Z",
     "start_time": "2025-09-01T05:42:30.901440Z"
    }
   },
   "source": [
    "from frameworks.lstm_sentiment import LSTMClassifier\n",
    "# Training loop (reusing from Step 5)\n",
    "from frameworks.lstm_sentiment import train_model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Corrected Data Loading ---\n",
    "\n",
    "    # 1. Load the dataset iterators ONCE\n",
    "    train_iter_vocab, _ = IMDB(split=(\"train\", \"test\"))\n",
    "\n",
    "    # 2. Build the vocabulary from the training iterator\n",
    "    vocab = build_vocab(train_iter_vocab)\n",
    "\n",
    "    # 3. Re-initialize the iterators to use them for the DataLoader\n",
    "    train_iter, test_iter = IMDB(split=(\"train\", \"test\"))\n",
    "\n",
    "    # 4. Define the collate function and create DataLoaders\n",
    "    # Note: In your lstm_sentiment.py, preprocess_data takes vocab as an argument.\n",
    "    # I've corrected it here.\n",
    "    collate_fn = preprocess_data(vocab)\n",
    "    train_loader = DataLoader(list(train_iter), batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(list(test_iter), batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # --- Rest of your code ---\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # LSTM\n",
    "    from frameworks.lstm_sentiment import train_model, LSTMClassifier\n",
    "    lstm_model = LSTMClassifier(len(vocab)).to(device)\n",
    "    lstm_losses, lstm_accs = train_model(lstm_model, train_loader, test_loader, epochs=2, device=device)\n",
    "\n",
    "    # GRU (Assuming GRUClassifier is defined elsewhere)\n",
    "    gru_model = GRUClassifier(len(vocab)).to(device)\n",
    "    gru_losses, gru_accs = train_model(gru_model, train_loader, test_loader, epochs=2, device=device)\n",
    "    \n",
    "    # Plot accuracy comparison\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(lstm_accs, label=\"LSTM\")\n",
    "    ax.plot(gru_accs, label=\"GRU\")\n",
    "    ax.set_title(\"GRU vs LSTM on IMDB\")\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.legend()\n",
    "    save_plot(fig, \"gru_vs_lstm.png\")"
   ],
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "IMDB.__init__() missing 3 required positional arguments: 'path', 'text_field', and 'label_field'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 9\u001B[39m\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mframeworks\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mlstm_sentiment\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m train_model\n\u001B[32m      5\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[34m__name__\u001B[39m == \u001B[33m\"\u001B[39m\u001B[33m__main__\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m      6\u001B[39m     \u001B[38;5;66;03m# --- Corrected Data Loading ---\u001B[39;00m\n\u001B[32m      7\u001B[39m \n\u001B[32m      8\u001B[39m     \u001B[38;5;66;03m# 1. Load the dataset iterators ONCE\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m9\u001B[39m     train_iter_vocab, _ = \u001B[43mIMDB\u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit\u001B[49m\u001B[43m=\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtrain\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtest\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     11\u001B[39m     \u001B[38;5;66;03m# 2. Build the vocabulary from the training iterator\u001B[39;00m\n\u001B[32m     12\u001B[39m     vocab = build_vocab(train_iter_vocab)\n",
      "\u001B[31mTypeError\u001B[39m: IMDB.__init__() missing 3 required positional arguments: 'path', 'text_field', and 'label_field'"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
